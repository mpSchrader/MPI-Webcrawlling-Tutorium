{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Higher Level Webcrawling (Solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example we create a web crawler from scratch. Now we will use the propably most used Webcrawling Framework [Scrapy](https://scrapy.org/) to do the same thing. <br>\n",
    "Usually Scrapy is run via the command line not in a Notebook, but for the workshop we will use a small hack to run it in the terminal. For a tutorial on how to run Scrapy regularly, see this [tutorial](https://doc.scrapy.org/en/latest/intro/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.exporters import JsonItemExporter\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we setup a Pipeline to store all articles to the articles_pipeline.json file <br>\n",
    "The JSONWriterPipeline is a simple element, which receives an crawled article and stores it into the `articles_pipline.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.file = open(\"articles_pipeline.json\", 'wb')\n",
    "        self.exporter = JsonItemExporter(self.file, encoding='utf-8', ensure_ascii=False)\n",
    "        self.exporter.start_exporting()\n",
    " \n",
    "    def close_spider(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    " \n",
    "    def process_item(self, item, spider):\n",
    "        self.exporter.export_item(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spider\n",
    "A spider is the core of a Scrapy crawler. Every spider needs a list of starting urls. `start_urls` and needs to implement the message `parse(self, response)`. \n",
    "To access the single elements in a website uses [XPath]() or [css](https://doc.scrapy.org/en/latest/topics/selectors.html). A few fundamential examples for the [XPath syntax](https://www.w3schools.com/xml/xpath_syntax.asp) or:\n",
    "- XPath: Select the text of a paragraph based with a special id: `//a[@id=\"author-id\"]/text()`\n",
    "- CSS: Get the href of a link, with a specific class: `a.myclass::attr(href)`\n",
    "- CSS: Get the text of a headline with a specific class: `h1.myheadline::text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArticleSpider(scrapy.Spider):\n",
    "    \"\"\"\n",
    "    Crawls the all articles published by TechCrunch.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'articles'\n",
    "\n",
    "    start_urls = ['https://techcrunch.com/']\n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',\n",
    "        'FEED_URI': 'articles.json'\n",
    "    }\n",
    "    \n",
    "    pagination_count = 0\n",
    "    max_pages = 3 # Maximum number of pages, which list the articles\n",
    "        \n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        Crawls all pages listing the articles.\n",
    "        \"\"\"\n",
    "        print(\"Starting Crawling: {}\".format(response.url))\n",
    "        # follow links to article pages\n",
    "        for href in response.css('a.read-more::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        self.pagination_count += 1\n",
    "        if self.pagination_count < self.max_pages:\n",
    "            # follow pagination links\n",
    "            next_href = response.css('li.next a::attr(href)').extract_first()\n",
    "            next_page = response.urljoin(next_href)\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        \"\"\"\n",
    "        Extracts information for a given article.\n",
    "        \"\"\"\n",
    "        title   = response.css('h1.tweet-title::text').extract_first().strip()\n",
    "        authors = response.xpath('//a[@rel=\"author\"]/text()').extract()\n",
    "        date    = response.css('time::attr(datetime)').extract_first()\n",
    "        tags    = response.css('a.tag::text').extract()\n",
    "        \n",
    "        text_raw = response.css('div.text p::text').extract()\n",
    "        text = ' '.join(text_raw)\n",
    "        \n",
    "\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'date': date,\n",
    "            'tags': tags,\n",
    "            'text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Crawling\n",
    "To start crawling we start a crawler process which uses our ArticleSpider to crawl TechCrunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-04 19:31:44 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
      "2018-03-04 19:31:44 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7ff155aac080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Crawling: https://techcrunch.com/\n",
      "Starting Crawling: https://techcrunch.com/page/2/\n",
      "Starting Crawling: https://techcrunch.com/page/3/\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(ArticleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data\n",
    "We can use Pandas to load the JSON file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Josh Constine]</td>\n",
       "      <td>2018-03-02 13:30:57</td>\n",
       "      <td>[Apps, Snapchat, Evan Spiegel, snap inc, snapc...</td>\n",
       "      <td>“Timing”, Snapchat CEO Evan Spiegel said crypt...</td>\n",
       "      <td>Snapchat is stuck in the uncanny valley of AR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Jonathan Salama]</td>\n",
       "      <td>2018-03-02 07:45:38</td>\n",
       "      <td>[Transportation, trucking]</td>\n",
       "      <td>\\n Jonathan Salama is chief technology officer...</td>\n",
       "      <td>Blockchain will work in trucking — but only if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Sarah Perez]</td>\n",
       "      <td>2018-03-02 10:53:10</td>\n",
       "      <td>[Apps, iphone apps, storage, iOS apps, Apps]</td>\n",
       "      <td>These days, home movies aren’t recorded with h...</td>\n",
       "      <td>Air’s app lets you record high-quality home mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Devin Coldewey]</td>\n",
       "      <td>2018-03-03 17:05:20</td>\n",
       "      <td>[eCommerce, Amazon, counterfeit]</td>\n",
       "      <td>It’s become a standard part of my dwindling Am...</td>\n",
       "      <td>Another small business complains of counterfei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Danny Crichton]</td>\n",
       "      <td>2018-03-04 09:17:01</td>\n",
       "      <td>[Government, Facebook, Google]</td>\n",
       "      <td>If there is one policy dilemma facing nearly e...</td>\n",
       "      <td>No one wants to build a “feel good” internet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             authors                date  \\\n",
       "0    [Josh Constine] 2018-03-02 13:30:57   \n",
       "1  [Jonathan Salama] 2018-03-02 07:45:38   \n",
       "2      [Sarah Perez] 2018-03-02 10:53:10   \n",
       "3   [Devin Coldewey] 2018-03-03 17:05:20   \n",
       "4   [Danny Crichton] 2018-03-04 09:17:01   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [Apps, Snapchat, Evan Spiegel, snap inc, snapc...   \n",
       "1                         [Transportation, trucking]   \n",
       "2       [Apps, iphone apps, storage, iOS apps, Apps]   \n",
       "3                   [eCommerce, Amazon, counterfeit]   \n",
       "4                     [Government, Facebook, Google]   \n",
       "\n",
       "                                                text  \\\n",
       "0  “Timing”, Snapchat CEO Evan Spiegel said crypt...   \n",
       "1  \\n Jonathan Salama is chief technology officer...   \n",
       "2  These days, home movies aren’t recorded with h...   \n",
       "3  It’s become a standard part of my dwindling Am...   \n",
       "4  If there is one policy dilemma facing nearly e...   \n",
       "\n",
       "                                               title  \n",
       "0  Snapchat is stuck in the uncanny valley of AR ...  \n",
       "1  Blockchain will work in trucking — but only if...  \n",
       "2  Air’s app lets you record high-quality home mo...  \n",
       "3  Another small business complains of counterfei...  \n",
       "4       No one wants to build a “feel good” internet  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfjson = pd.read_json('articles_pipeline.json')\n",
    "print(dfjson.shape)\n",
    "dfjson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
