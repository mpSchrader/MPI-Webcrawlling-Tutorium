{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Low Level Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will crawl [TechCrunch.com](https://techcrunch.com) as an example. Please note that this is only an example and should not be used in any commercial context or something similar, to not violate TechCrunches terms and conditions.\n",
    "<br>\n",
    "<br>\n",
    "To implement our low level crawler we will only use basic Python packages, such as [requests](http://docs.python-requests.org/en/master/), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), and [Pandas](https://pandas.pydata.org/pandas-docs/stable/). Firstly, we will implement the individual components of the webcrawler as functions. Secondly, we will combine everything to functional webcrawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "The preparation consists of importing all needed packages and defining the basic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://techcrunch.com'\n",
    "number_of_pages = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the webpage for one url\n",
    "First we will implement a simple function, which gets an URL and returns a BeautifulSoup-Object. Therefor we will send a GET request and parse the response text to the BeautifulSoup-Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # TODO:\n",
    "    #   - Get HTML Page\n",
    "    #   - Check Status\n",
    "    #   - Convert to BeautifulSoup\n",
    "    # HTTP Get request \n",
    "    \n",
    "    page = None\n",
    "    \n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = get_page(base_url)\n",
    "page.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article URLs\n",
    "Now we need to find all URLs to the articles listed on the page we previously retrieved. We can find the urls in the **Read More** buttons.\n",
    "![Example Article](img/article.png)\n",
    "Each link has a structure like this:\n",
    "```HTML \n",
    "<a href=\"https://techcrunch.com/2018/03/02/some-random-article/\" \n",
    "   class=\"read-more\" \n",
    "   data-omni-sm=\"gbl_river_readmore,2\">\n",
    "        Read More\n",
    "</a>```\n",
    "To identfiy all relevant links we can use BeautifullSoup's find all function, which allows us also to filter for specific classes. In our case the class is called **read-more**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(page):\n",
    "    # Get a list of all links of class read-more\n",
    "    \n",
    "    refs = []\n",
    "    \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://techcrunch.com/page/2/'\n",
    "page = get_page(url)\n",
    "article_urls = get_article_urls(page)\n",
    "article_urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article Info\n",
    "In this step we will implement a function, which extracts all wanted information for one article url. You will need to implement:\n",
    "1. Get the page (Hint: You can use already implemented functions)\n",
    "2. Extract all desired information (Title, Authors, Date, Tags, Text)\n",
    "3. Combine all in a dictionary\n",
    "\n",
    "The extraction of the information works kind of similar to previous code. You just need `page.find(...)` and `page.find_all(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_info(url, delay=1):\n",
    "\n",
    "    article = None\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://techcrunch.com/2018/03/02/2018-party-and-sxsw-panels/'\n",
    "article = get_article_info(url, delay=0)\n",
    "# Shorten text for better readability. Not needed in the real crawler.\n",
    "article['text'] = article['text'][:300] + '...'\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the next URL\n",
    "Finally we need to extraxt the URL of the next page listing articles. We will use the same procedure as used before to find the href with the text next.<br>\n",
    "![Next Button](img/next_button.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_url(page, base_url):\n",
    "    \n",
    "    # TODO: Extract next url\n",
    "    \n",
    "    url = 'https://www.notthispage.com'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_next_url(page, base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together\n",
    "Now we implemented all the important parts, we need to run the crawler. The last challenge is to put them together. Therfor we will run through **number_of_pages** pages, which list recent articles on TechCrunch, by:\n",
    "1. Get the page for the current URL\n",
    "2. Extract the article URLs for each page.\n",
    "3. Get the information for every article.\n",
    "4. Add the article information to the list articles, containg the information for all articles.\n",
    "5. Find reference to next page listing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_url = base_url\n",
    "\n",
    "articles = []\n",
    "for n in range(number_of_pages):\n",
    "    \n",
    "    print('Crawling: {}'.format(current_url))    \n",
    "    \n",
    "    # 1. Get the page for the current URL    \n",
    "    # 2. Extract the article URLs for each page.\n",
    "    # 3. Get the information for every article.\n",
    "    # 4. Add the article information to the list articles, containg the information for all articles.\n",
    "    # 5. Find reference to next page listing articles\n",
    "   \n",
    "    \n",
    "print('Finished crawling. Found {} Articles'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results to csv\n",
    "To store the our articles we use the library [Pandas](https://pandas.pydata.org/pandas-docs/stable/), which is the default library for Python to handle dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = # TODO Create dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Store data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this section we learned how to write a basic webcrawler, which gets a starting URL and explores the articles published on TechCrunch in a given pattern. The webcrawler we developed is a really simple one. You could enhence the webcrawler e.g. by:\n",
    "- Storing all files HTML files [Reference](https://www.digitalocean.com/community/tutorials/how-to-handle-plain-text-files-in-python-3)\n",
    "- Add logging to your code [Reference](https://docs.python.org/3/howto/logging-cookbook.html)\n",
    "- Filter the pages, e.g. to only collect articles with the tag _Artificial Intelligence_\n",
    "\n",
    "### Questions?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
