{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Webcrawling Tutorium (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to web crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Low Level Crawling\n",
    "In the following we will crawl [TechCrunch.com](https://techcrunch.com) as an example. Please note that this is only an example and should not be used in any commercial context or something similar, to not violate TechCrunches terms and conditions.\n",
    "<br>\n",
    "<br>\n",
    "To implement our low level crawler we will only use basic Python packages, such as [requests](http://docs.python-requests.org/en/master/), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), and [Pandas](https://pandas.pydata.org/pandas-docs/stable/). Firstly, we will implement the individual components of the webcrawler as functions. Secondly, we will combine everything to functional webcrawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "The preparation consists of importing all needed packages and defining the basic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://techcrunch.com'\n",
    "number_of_pages = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the webpage for one url\n",
    "First we will implement a simple function, which gets an URL and returns a BeautifulSoup-Object. Therefor we will send a GET request and parse the response text to the BeautifulSoup-Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Simple error creation, just stop execution when no proper response received\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError('Error getting page {}'.format(current_url))\n",
    "        \n",
    "    # Converting raw response text to usable BeautifulSoup\n",
    "    page = bs4.BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article URLs\n",
    "Now we need to find all URLs to the articles listed on the page we previously retrieved. We can find the urls in the **Read More** buttons.\n",
    "![Example Article](img/article.png)\n",
    "Each link has a structure like this:\n",
    "```HTML \n",
    "<a href=\"https://techcrunch.com/2018/03/02/some-random-article/\" \n",
    "   class=\"read-more\" \n",
    "   data-omni-sm=\"gbl_river_readmore,2\">\n",
    "        Read More\n",
    "</a>```\n",
    "To identfiy all relevant links we can use BeautifullSoup's find all function, which allows us also to filter for specific classes. In our case the class is called **read-more**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls(page):\n",
    "    a_s = page.find_all('a', {'class': 'read-more'})\n",
    "    \n",
    "    hrefs = []\n",
    "    for a in a_s:\n",
    "        hrefs.append(a.attrs['href'])\n",
    "        \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://techcrunch.com/2018/03/02/2018-party-and-sxsw-panels/',\n",
       " 'https://techcrunch.com/2018/03/02/air-lets-you-record-high-quality-home-movies-without-running-out-of-space/',\n",
       " 'https://techcrunch.com/2018/03/02/blockchain-will-work-in-trucking-but-only-if-these-three-things-happen/']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://techcrunch.com/page/2/'\n",
    "article_urls = get_article_urls(page)\n",
    "article_urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article Info\n",
    "In this step we will implement a function, which extracts all wanted information for one article url. You will need to implement:\n",
    "1. Get the page (Hint: You can use already implemented functions)\n",
    "2. Extract all desired information (Title, Authors, Date, Tags, Text)\n",
    "3. Combine all in a dictionary\n",
    "\n",
    "The extraction of the information works kind of similar to previous code. You just need `page.find(...)` and `page.find_all(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_info(url, delay=1):\n",
    "    \n",
    "    # Wait for delay seconds to crawl the next page\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    page = get_page(url)\n",
    "    \n",
    "    # Exctract Information\n",
    "    title = page.find('h1', {'class': 'tweet-title'}).text\n",
    "    \n",
    "    authors_raw = page.find_all('a', {'rel': 'author'})\n",
    "    authors = [author.text for author in authors_raw]\n",
    "    \n",
    "    date = page.find('time').attrs['datetime']\n",
    "    \n",
    "    tags_raw = page.find_all('a', {'class': 'tag'})\n",
    "    tags = [tag.get_text(strip=True) for tag in tags_raw]\n",
    "    \n",
    "    # Get the text. The two staged filtering is needed, because \n",
    "    # in some articles div.text contains also scripts and adds, which we don't want to include.\n",
    "    # The relevant text can be found in all p tags in text_raw.\n",
    "    text_raw = page.find('div', {'class': 'text'})\n",
    "    text_raw = [t.get_text(strip=True) for t in text_raw.find_all('p')]\n",
    "    text = ' '.join(text_raw)\n",
    "    \n",
    "    # Combine all information in one set\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date': date,\n",
    "        'authors': authors,\n",
    "        'tags': tags,\n",
    "        'text': text\n",
    "    }\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['Josh Constine'],\n",
       " 'date': '2018-03-02 03:23:27',\n",
       " 'tags': ['Entertainment', 'SXSW', 'TechCrunch'],\n",
       " 'text': 'TechCrunch invites you to our annual Crunch By Crunch Fest party in Austin, Texas.RSVPto come meet our writers while enjoying free drinks and musical performances by live electronic pop wizardsAutograf, digital RnB drummer Mobley, angelic songwriter MIEARS, and yacht dance DJs Glassio. It’s going do...',\n",
       " 'title': 'Come to TechCrunch’s party and SXSW panels',\n",
       " 'url': 'https://techcrunch.com/2018/03/02/2018-party-and-sxsw-panels/'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://techcrunch.com/2018/03/02/2018-party-and-sxsw-panels/'\n",
    "article = get_article_info(url, delay=0)\n",
    "# Shorten text for better readability. Not needed in the real crawler.\n",
    "article['text'] = article['text'][:300] + '...'\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the next URL\n",
    "Finally we need to extraxt the URL of the next page listing articles. We will use the same procedure as used before to find the href with the text next.<br>\n",
    "![Next Button](img/next_button.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_url(page, base_url):\n",
    "    \n",
    "    list_item = page.find('li', {'class': 'next'})\n",
    "    href = list_item.find('a').attrs['href']\n",
    "    \n",
    "    url = base_url + href\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together\n",
    "Now we implemented all the important parts, we need to run the crawler. The last challenge is to put them together. Therfor we will run through **number_of_pages** pages, which list recent articles on TechCrunch, by:\n",
    "1. Get the page for the current URL\n",
    "2. Extract the article URLs for each page.\n",
    "3. Get the information for every article.\n",
    "4. Add the article information to the list articles, containg the information for all articles.\n",
    "5. Get the URL for the next page, which lists the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://techcrunch.com\n",
      "Finished crawling. Found 19 Articles\n"
     ]
    }
   ],
   "source": [
    "current_url = base_url\n",
    "\n",
    "articles = []\n",
    "for n in range(number_of_pages):\n",
    "    \n",
    "    print('Crawling: {}'.format(current_url))    \n",
    "    \n",
    "    # Get a beautiful soup object representing the current page\n",
    "    page = get_page(current_url)\n",
    "    \n",
    "    article_urls = get_article_urls(page)\n",
    "    # Run through all articles and extract the desired information\n",
    "    for url in article_urls:\n",
    "        try:\n",
    "            article_info = get_article_info(url, delay=0.3)\n",
    "        except:\n",
    "            print('Error for article: {}'.format(url))\n",
    "        articles.append(article_info)\n",
    "        \n",
    "    # Find reference to next page listing articles\n",
    "    current_url = get_next_url(page, base_url)\n",
    "    \n",
    "print('Finished crawling. Found {} Articles'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results to csv\n",
    "To store the our articles we use the library [Pandas](https://pandas.pydata.org/pandas-docs/stable/), which is the default library for Python to handle dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('my_crawled_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this section we learned how to write a basic webcrawler, which gets a starting URL and explores the articles published on TechCrunch in a given pattern. The webcrawler we developed is a really simple one. You could enhence the webcrawler e.g. by:\n",
    "- Storing all files HTML files [Reference](https://www.digitalocean.com/community/tutorials/how-to-handle-plain-text-files-in-python-3)\n",
    "- Add logging to your code [Reference](https://docs.python.org/3/howto/logging-cookbook.html)\n",
    "- Filter the pages, e.g. to only collect articles with the tag _Artificial Intelligence_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Higher Level Webcrawling\n",
    "In the first example we create a web crawler from scratch. Now we will use the propably most used Webcrawling Framework [Scrapy](https://scrapy.org/) to do the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
