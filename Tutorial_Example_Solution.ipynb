{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Webcrawling Tutorium (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to web crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Low Level Crawling\n",
    "In the following we will crawl [TechCrunch.com](https://techcrunch.com) as an example. Please note that this is only an example and should not be used in any commercial context or something similar, to not violate TechCrunches terms and conditions.\n",
    "<br>\n",
    "<br>\n",
    "To implement our low level crawler we will only use basic Python packages, such as [requests](http://docs.python-requests.org/en/master/), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), and [Pandas](https://pandas.pydata.org/pandas-docs/stable/). Firstly, we will implement the individual components of the webcrawler as functions. Secondly, we will combine everything to functional webcrawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "The preparation consists of importing all needed packages and defining the basic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://techcrunch.com'\n",
    "number_of_pages = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the webpage for one url\n",
    "First we will implement a simple function, which gets an URL and returns a BeautifulSoup-Object. Therefor we will send a GET request and parse the response text to the BeautifulSoup-Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # HTTP Get request \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Simple error creation, just stop execution when no proper response received\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError('Error getting page {}'.format(current_url))\n",
    "        \n",
    "    # Converting raw response text to usable BeautifulSoup\n",
    "    page = bs4.BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = get_page(base_url)\n",
    "page.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article URLs\n",
    "Now we need to find all URLs to the articles listed on the page we previously retrieved. We can find the urls in the **Read More** buttons.\n",
    "![Example Article](img/article.png)\n",
    "Each link has a structure like this:\n",
    "```HTML \n",
    "<a href=\"https://techcrunch.com/2018/03/02/some-random-article/\" \n",
    "   class=\"read-more\" \n",
    "   data-omni-sm=\"gbl_river_readmore,2\">\n",
    "        Read More\n",
    "</a>```\n",
    "To identfiy all relevant links we can use BeautifullSoup's find all function, which allows us also to filter for specific classes. In our case the class is called **read-more**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(page):\n",
    "    # Get a list of all links of class read-more\n",
    "    a_s = page.find_all('a', {'class': 'read-more'})\n",
    "    \n",
    "    hrefs = []\n",
    "    # Extract the href URLs for every a in a_s\n",
    "    for a in a_s:\n",
    "        hrefs.append(a.attrs['href'])\n",
    "        \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://techcrunch.com/page/2/'\n",
    "page = get_page(url)\n",
    "article_urls = get_article_urls(page)\n",
    "article_urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article Info\n",
    "In this step we will implement a function, which extracts all wanted information for one article url. You will need to implement:\n",
    "1. Get the page (Hint: You can use already implemented functions)\n",
    "2. Extract all desired information (Title, Authors, Date, Tags, Text)\n",
    "3. Combine all in a dictionary\n",
    "\n",
    "The extraction of the information works kind of similar to previous code. You just need `page.find(...)` and `page.find_all(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_info(url, delay=1):\n",
    "    \n",
    "    # Wait for delay seconds to crawl the next page\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    page = get_page(url)\n",
    "    \n",
    "    # Exctract Information\n",
    "    title = page.find('h1', {'class': 'tweet-title'}).text\n",
    "    \n",
    "    authors_raw = page.find_all('a', {'rel': 'author'})\n",
    "    authors = [author.text for author in authors_raw]\n",
    "    \n",
    "    date = page.find('time').attrs['datetime']\n",
    "    \n",
    "    tags_raw = page.find_all('a', {'class': 'tag'})\n",
    "    tags = [tag.get_text(strip=True) for tag in tags_raw]\n",
    "    \n",
    "    # Get the text. The two staged filtering is needed, because \n",
    "    # in some articles div.text contains also scripts and adds, which we don't want to include.\n",
    "    # The relevant text can be found in all p tags in text_raw.\n",
    "    text_raw = page.find('div', {'class': 'text'})\n",
    "    text_raw = [t.get_text(strip=True) for t in text_raw.find_all('p')]\n",
    "    text = ' '.join(text_raw)\n",
    "    \n",
    "    # Combine all information in one set\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date': date,\n",
    "        'authors': authors,\n",
    "        'tags': tags,\n",
    "        'text': text\n",
    "    }\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://techcrunch.com/2018/03/02/2018-party-and-sxsw-panels/'\n",
    "article = get_article_info(url, delay=0)\n",
    "# Shorten text for better readability. Not needed in the real crawler.\n",
    "article['text'] = article['text'][:300] + '...'\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the next URL\n",
    "Finally we need to extraxt the URL of the next page listing articles. We will use the same procedure as used before to find the href with the text next.<br>\n",
    "![Next Button](img/next_button.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_url(page, base_url):\n",
    "    \n",
    "    list_item = page.find('li', {'class': 'next'})\n",
    "    href = list_item.find('a').attrs['href']\n",
    "    \n",
    "    url = base_url + href\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_url(page, base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together\n",
    "Now we implemented all the important parts, we need to run the crawler. The last challenge is to put them together. Therfor we will run through **number_of_pages** pages, which list recent articles on TechCrunch, by:\n",
    "1. Get the page for the current URL\n",
    "2. Extract the article URLs for each page.\n",
    "3. Get the information for every article.\n",
    "4. Add the article information to the list articles, containg the information for all articles.\n",
    "5. Find reference to next page listing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_url = base_url\n",
    "\n",
    "articles = []\n",
    "for n in range(number_of_pages):\n",
    "    \n",
    "    print('Crawling: {}'.format(current_url))    \n",
    "    \n",
    "    # 1. Get the page for the current URL\n",
    "    page = get_page(current_url)\n",
    "    \n",
    "    # 2. Extract the article URLs for each page.\n",
    "    article_urls = get_article_urls(page)\n",
    "    \n",
    "    # Run through all articles and extract the desired information\n",
    "    for url in article_urls:\n",
    "        \n",
    "        try:\n",
    "            # 3. Get the information for every article.\n",
    "            article_info = get_article_info(url, delay=0.3)\n",
    "        except:\n",
    "            print('Error for article: {}'.format(url))\n",
    "            \n",
    "        # 4. Add the article information to the list articles, containg the information for all articles.\n",
    "        articles.append(article_info)\n",
    "        \n",
    "    # 5. Find reference to next page listing articles\n",
    "    current_url = get_next_url(page, base_url)\n",
    "    \n",
    "print('Finished crawling. Found {} Articles'.format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results to csv\n",
    "To store the our articles we use the library [Pandas](https://pandas.pydata.org/pandas-docs/stable/), which is the default library for Python to handle dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('my_crawled_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this section we learned how to write a basic webcrawler, which gets a starting URL and explores the articles published on TechCrunch in a given pattern. The webcrawler we developed is a really simple one. You could enhence the webcrawler e.g. by:\n",
    "- Storing all files HTML files [Reference](https://www.digitalocean.com/community/tutorials/how-to-handle-plain-text-files-in-python-3)\n",
    "- Add logging to your code [Reference](https://docs.python.org/3/howto/logging-cookbook.html)\n",
    "- Filter the pages, e.g. to only collect articles with the tag _Artificial Intelligence_\n",
    "\n",
    "### Questions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Higher Level Webcrawling\n",
    "In the first example we create a web crawler from scratch. Now we will use the propably most used Webcrawling Framework [Scrapy](https://scrapy.org/) to do the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.exporters import JsonItemExporter\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.file = open(\"articles_pipeline.json\", 'wb')\n",
    "        self.exporter = JsonItemExporter(self.file, encoding='utf-8', ensure_ascii=False)\n",
    "        self.exporter.start_exporting()\n",
    " \n",
    "    def close_spider(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    " \n",
    "    def process_item(self, item, spider):\n",
    "        self.exporter.export_item(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'articles'\n",
    "\n",
    "    start_urls = ['https://techcrunch.com/']\n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',\n",
    "        'FEED_URI': 'articles.json'\n",
    "    }\n",
    "    \n",
    "    pagination_count = 0\n",
    "    max_pages = 3\n",
    "        \n",
    "    def parse(self, response):\n",
    "        print(\"Starting Crawling: {}\".format(response.url))\n",
    "        # follow links to article pages\n",
    "        for href in response.css('a.read-more::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        self.pagination_count += 1\n",
    "        if self.pagination_count < self.max_pages:\n",
    "            # follow pagination links\n",
    "            next_href = response.css('li.next a::attr(href)').extract_first()\n",
    "            next_page = response.urljoin(next_href)\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "\n",
    "        title   = response.css('h1.tweet-title::text').extract_first().strip()\n",
    "        authors = response.xpath('//a[@rel=\"author\"]/text()').extract()\n",
    "        date    = response.css('time::attr(datetime)').extract_first()\n",
    "        tags    = response.css('a.tag::text').extract()\n",
    "        \n",
    "        text_raw = response.css('div.text p::text').extract()\n",
    "        text = ' '.join(text_raw)\n",
    "        \n",
    "\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'date': date,\n",
    "            'tags': tags,\n",
    "            'text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-04 19:31:44 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
      "2018-03-04 19:31:44 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7ff155aac080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Crawling: https://techcrunch.com/\n",
      "Starting Crawling: https://techcrunch.com/page/2/\n",
      "Starting Crawling: https://techcrunch.com/page/3/\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(ArticleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Josh Constine]</td>\n",
       "      <td>2018-03-02 13:30:57</td>\n",
       "      <td>[Apps, Snapchat, Evan Spiegel, snap inc, snapc...</td>\n",
       "      <td>“Timing”, Snapchat CEO Evan Spiegel said crypt...</td>\n",
       "      <td>Snapchat is stuck in the uncanny valley of AR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Jonathan Salama]</td>\n",
       "      <td>2018-03-02 07:45:38</td>\n",
       "      <td>[Transportation, trucking]</td>\n",
       "      <td>\\n Jonathan Salama is chief technology officer...</td>\n",
       "      <td>Blockchain will work in trucking — but only if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Sarah Perez]</td>\n",
       "      <td>2018-03-02 10:53:10</td>\n",
       "      <td>[Apps, iphone apps, storage, iOS apps, Apps]</td>\n",
       "      <td>These days, home movies aren’t recorded with h...</td>\n",
       "      <td>Air’s app lets you record high-quality home mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Devin Coldewey]</td>\n",
       "      <td>2018-03-03 17:05:20</td>\n",
       "      <td>[eCommerce, Amazon, counterfeit]</td>\n",
       "      <td>It’s become a standard part of my dwindling Am...</td>\n",
       "      <td>Another small business complains of counterfei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Danny Crichton]</td>\n",
       "      <td>2018-03-04 09:17:01</td>\n",
       "      <td>[Government, Facebook, Google]</td>\n",
       "      <td>If there is one policy dilemma facing nearly e...</td>\n",
       "      <td>No one wants to build a “feel good” internet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             authors                date  \\\n",
       "0    [Josh Constine] 2018-03-02 13:30:57   \n",
       "1  [Jonathan Salama] 2018-03-02 07:45:38   \n",
       "2      [Sarah Perez] 2018-03-02 10:53:10   \n",
       "3   [Devin Coldewey] 2018-03-03 17:05:20   \n",
       "4   [Danny Crichton] 2018-03-04 09:17:01   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [Apps, Snapchat, Evan Spiegel, snap inc, snapc...   \n",
       "1                         [Transportation, trucking]   \n",
       "2       [Apps, iphone apps, storage, iOS apps, Apps]   \n",
       "3                   [eCommerce, Amazon, counterfeit]   \n",
       "4                     [Government, Facebook, Google]   \n",
       "\n",
       "                                                text  \\\n",
       "0  “Timing”, Snapchat CEO Evan Spiegel said crypt...   \n",
       "1  \\n Jonathan Salama is chief technology officer...   \n",
       "2  These days, home movies aren’t recorded with h...   \n",
       "3  It’s become a standard part of my dwindling Am...   \n",
       "4  If there is one policy dilemma facing nearly e...   \n",
       "\n",
       "                                               title  \n",
       "0  Snapchat is stuck in the uncanny valley of AR ...  \n",
       "1  Blockchain will work in trucking — but only if...  \n",
       "2  Air’s app lets you record high-quality home mo...  \n",
       "3  Another small business complains of counterfei...  \n",
       "4       No one wants to build a “feel good” internet  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfjson = pd.read_json('articles_pipeline.json')\n",
    "print(dfjson.shape)\n",
    "dfjson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
